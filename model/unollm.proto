syntax = "proto3";

package unoLLM;
option go_package = "./model";

message LLMTokenCount {
  int64 total_token = 1;
  int64 prompt_token = 2;
  int64 completion_token = 3;
}

message LLMChatCompletionMessage {
  string role = 1;
  string content = 2;
}

message LLMRequestInfo {
  string llm_api_type = 1;
  string model = 2;
  double temperature = 3;
  double top_p = 4;
  double top_k = 5;
  string url = 6;
  string token = 7;
  map<string, string> metainfo = 8;
}

message LLMRequestSchema {
  repeated LLMChatCompletionMessage messages = 1;
  LLMRequestInfo llm_request_info = 2;
}

message LLMResponseSchema {
  LLMChatCompletionMessage message = 1;
  LLMTokenCount llm_token_count = 2;
}


message Done {
}

message PartialLLMResponse {
    oneof response {
        string content = 1;
        Done done = 2;
    }
    optional LLMTokenCount llm_token_count = 3;
}

service UnoLLMv1 {
    rpc BlockingRequestLLM(LLMRequestSchema) returns (LLMResponseSchema) {}
    rpc StreamRequestLLM(LLMRequestSchema) returns (stream PartialLLMResponse) {}
}


message EmbeddingResponse {
  EmbeddingRequestInfo embedding_request_info = 1;
  int32 dimension = 2;
  repeated float vectors = 3;
}

message EmbeddingRequestInfo {
  string llm_api_type = 1;
  string model = 2;
  string url = 3;
  string token = 4;
}

message EmbeddingRequest {
  EmbeddingRequestInfo embedding_request_info = 1;
  string text = 2;
}

service UnoEmbeddingv1 {
  rpc EmbeddingRequestLLM(EmbeddingRequest) returns (EmbeddingResponse) {}
}
